vnf = rbindlist(
list(vnf,
vnf.temp))
# output every 50 days. updating every day would be pretty slow
if(d %% 50 == 0 | d == length(vnf.dates)){
saveRDS(vnf, paste0(data.dir, "ef_vnf/ef-vnf.rds"))
}
}
View(vnf)
pb.vnf = paste0(data.dir, "pb_vnf/pb-vnf.rds") %>% readRDS() %>%
# Filter for detections with strongest confidence (cloud mask = 0)
filter(cloud_mask == 0) %>%
# select and rename some columns
dplyr::select(vnf_id, date, vnf_lon = lon, vnf_lat = lat,
temp_bb, area_bb, rhi) %>%
filter(!(date < "2016-01-01"))
# Read in entirety of VNF within Eagle Ford Shale
ef.vnf = paste0(data.dir, "ef_vnf/ef-vnf.rds") %>% readRDS() %>%
# Filter for detections with strongest confidence (cloud mask = 0)
filter(cloud_mask == 0) %>%
# select and rename some columns
dplyr::select(vnf_id, date, vnf_lon = lon, vnf_lat = lat,
temp_bb, area_bb, rhi) %>%
filter(!(date < "2016-01-01"))
# packages ----------------------------------------------------------------
pkgs = c('data.table', 'dbscan', 'sf', 'tidyverse')
for(p in pkgs) require(p, character.only = TRUE)
rm(p, pkgs)
# directories -------------------------------------------------------------
code.dir = paste0(getwd(), '/Downloads/STA496-Spatial-Analysis-on-UOGD-main/code/')
data.dir = paste0(getwd(), '/Downloads/STA496-Spatial-Analysis-on-UOGD-main/data/')
pkgs = c('data.table', 'R.utils', 'sf', 'tidyverse')
for(p in pkgs) require(p, character.only = TRUE)
rm(p, pkgs)
# directories -------------------------------------------------------------
source(paste0(code.dir, "helper_functions.R"))
View(pb.vnf)
pkgs = c('data.table', 'dbscan', 'sf', 'tidyverse')
for(p in pkgs) require(p, character.only = TRUE)
rm(p, pkgs)
code.dir = paste0(getwd(), '/Downloads/STA496-Spatial-Analysis-on-UOGD-main/code/')
data.dir = paste0(getwd(), '/Downloads/STA496-Spatial-Analysis-on-UOGD-main/data/')
pkgs = c('data.table', 'R.utils', 'sf', 'tidyverse')
for(p in pkgs) require(p, character.only = TRUE)
rm(p, pkgs)
ef_bbox = st_read(paste0(data.dir, "shapes/ShalePlays_US_EIA_Dec2021.shp"), quiet = TRUE) %>%
filter((Basin %in% c("Western Gulf"))) %>%
st_bbox()
lon_bounds = ef_bbox[c(1,3)]; lat_bounds = ef_bbox[c(2,4)]
# # see https://payneinstitute.mines.edu/eog-2/transition-to-secured-data-access/
pkgs = c("httr", "jsonlite", "utils")
for(p in pkgs) require(p, character.only = TRUE)
# Retrieve access token
# Retrieve access token
params <- list(
client_id = 'eogdata_oidc',
client_secret = '2677ad81-521b-4869-8480-6d05b9e57d48',
username = "jerryp.wu@mail.utoronto.ca",
password = "kBREu_D63qZ34g4",
grant_type = 'password'
)
token_url <- 'https://eogauth.mines.edu/auth/realms/master/protocol/openid-connect/token'
response <- POST(token_url, body = params, encode = "form")
access_token_list <- fromJSON(content(response,as="text",encoding="UTF-8"))
access_token <- access_token_list$access_token
vnfv21.url.pfx = c(
"https://eogdata.mines.edu/wwwdata/viirs_products/vnf/v21//VNF_npp_d",
"_noaa_v21.csv.gz")
vnfv30.url.pfx = c(
"https://eogdata.mines.edu/wwwdata/viirs_products/vnf/v30//VNF_npp_d",
"_noaa_v30-ez.csv.gz")
vnf.cols = c('date_mscan', 'lon_gmtco', 'lat_gmtco', 'temp_bb', 'temp_bkg',
'esf_bb', 'rhi', 'rh', 'area_pixel', 'area_bb', 'cloud_mask')
# Check existing VNF data -------------------------------------------------
vnf.data.exist = file.exists(paste0(data.dir, "ef_vnf/ef-vnf.rds"))
end_date = as.Date("2022-08-31")
if(vnf.data.exist){
vnf = readRDS(paste0(data.dir, "ef_vnf/ef-vnf.rds"))
# start new update from date after last date in existing data
start_date = max(vnf$file_date) + 1
} else {
vnf = data.table(
vnf_id = character(),
date = as.Date(character()),
lon = numeric(),
lat = numeric(),
temp_bb = integer(),
temp_bkg = integer(),
esf_bb = numeric(),
rhi = numeric(),
rh = numeric(),
area_pixel = numeric(),
area_bb = numeric(),
cloud_mask = integer(),
file_date = as.Date(character())
)
# start new update from date after last date in existing data
start_date = as.Date("2012-03-01")
}
if(start_date > end_date){
stop("Invalid dates for update.")
} else{
vnf.dates = seq(start_date, end_date, "days")
}
if(start_date > end_date){
stop("Invalid dates for update.")
} else{
vnf.dates = seq(start_date, end_date, "days")
}
# Download & update loop --------------------------------------------------
for(d in 1:length(vnf.dates)){
# prep ----
# starts timer to track download time
start_time = Sys.time()
# creates URL and GZ names, with date cutoff between V21 CLASS and V30 GRAVITE
if(vnf.dates[d] <= as.Date('2017-12-05')){
# V21 CLASS version
url.name = paste0(vnfv21.url.pfx[1],
gsub('-', '', vnf.dates[d]), vnfv21.url.pfx[2])
gz.name = paste0(data.dir, 'ef_vnf/raw/', basename(url.name))
} else{
# V30 GRAVITE version
url.name = paste0(vnfv30.url.pfx[1],
gsub('-', '', vnf.dates[d]), vnfv30.url.pfx[2])
gz.name = paste0(data.dir, 'ef_vnf/raw/', basename(url.name))
}
# get CSV name for unzipped file
csv.name = gsub(".gz", "", gz.name)
# download & process ----
# try to download the file, error handler in case of no data
tryCatch({
# message indicating date being downloaded
cat("[", as.character(vnf.dates[d]),'] download. ', sep='')
# downloads the file ----
download.file(url.name, gz.name,
mode = 'wb', quiet = TRUE,
headers = list(Authorization = auth))
# unzips GZ, keeps the CSV, removes the GZ
R.utils::gunzip(gz.name, overwrite = TRUE)
# process
cat('process. ', sep='')
# process the file ----
vnf.temp = fread(csv.name) %>%
rename_all(tolower) %>% # rename all columns to lowercase for convenience
select(all_of(vnf.cols)) %>% # collect relevant columns
na_if(999999) %>% # replace 999999 as missing
filter(!is.na(temp_bb)) %>% # keep those not missing temperature
# generate unique VNF_ID and valid date-format date column
mutate(vnf_id = paste0('VNF', gsub('-','',vnf.dates[d]), sprintf('%06d', 1:nrow(.))),
date = as.Date(substr(date_mscan, 1, 10), format = "%Y/%m/%d"),
file_date = vnf.dates[d]) %>%
# collect and rename certain columns, and drop a few
select(vnf_id, date, lon = lon_gmtco, lat = lat_gmtco,
everything(), -date_mscan) %>%
# filter by lat & lon bounds (bbox)
filter(lon >= lon_bounds[1], lon <= lon_bounds[2],
lat >= lat_bounds[1], lat <= lat_bounds[2])
# delete CSV file
file.remove(csv.name)
# report time took t to download and process ----
cat(difftime(Sys.time(), start_time, units='secs') %>% ceiling(),
" secs.\r", sep='')
}, error = function(e){
print(e)
})
# append to existing data
vnf = rbindlist(
list(vnf,
vnf.temp))
# output every 50 days. updating every day would be pretty slow
if(d %% 50 == 0 | d == length(vnf.dates)){
saveRDS(vnf, paste0(data.dir, "ef_vnf/ef-vnf.rds"))
}
}
eia.pb.vnf = pb.vnf %>% filter(temp_bb >= 1600)
# generate list of basins
basins = pb.ef$basin %>% unique()
eia.vnf.xy = list()
# Step 1: Filter VNF by > 1600K tempeture ---------------------------------
eia.pb.vnf = pb.vnf %>% filter(temp_bb >= 1600)
vnf.data.exist = file.exists(paste0(data.dir, "ef_vnf/ef-vnf.rds"))
end_date = as.Date("2022-08-31")
if(vnf.data.exist){
vnf = readRDS(paste0(data.dir, "ef_vnf/ef-vnf.rds"))
# start new update from date after last date in existing data
start_date = max(vnf$file_date) + 1
} else {
vnf = data.table(
vnf_id = character(),
date = as.Date(character()),
lon = numeric(),
lat = numeric(),
temp_bb = integer(),
temp_bkg = integer(),
esf_bb = numeric(),
rhi = numeric(),
rh = numeric(),
area_pixel = numeric(),
area_bb = numeric(),
cloud_mask = integer(),
file_date = as.Date(character())
)
# start new update from date after last date in existing data
start_date = as.Date("2012-03-01")
}
# vector of dates
if(start_date > end_date){
stop("Invalid dates for update.")
} else{
vnf.dates = seq(start_date, end_date, "days")
}
for(d in 1:length(vnf.dates)){
# prep ----
# starts timer to track download time
start_time = Sys.time()
# creates URL and GZ names, with date cutoff between V21 CLASS and V30 GRAVITE
if(vnf.dates[d] <= as.Date('2017-12-05')){
# V21 CLASS version
url.name = paste0(vnfv21.url.pfx[1],
gsub('-', '', vnf.dates[d]), vnfv21.url.pfx[2])
gz.name = paste0(data.dir, 'ef_vnf/raw/', basename(url.name))
} else{
# V30 GRAVITE version
url.name = paste0(vnfv30.url.pfx[1],
gsub('-', '', vnf.dates[d]), vnfv30.url.pfx[2])
gz.name = paste0(data.dir, 'ef_vnf/raw/', basename(url.name))
}
# get CSV name for unzipped file
csv.name = gsub(".gz", "", gz.name)
# download & process ----
# try to download the file, error handler in case of no data
tryCatch({
# message indicating date being downloaded
cat("[", as.character(vnf.dates[d]),'] download. ', sep='')
# downloads the file ----
download.file(url.name, gz.name,
mode = 'wb', quiet = TRUE,
headers = list(Authorization = auth))
# unzips GZ, keeps the CSV, removes the GZ
R.utils::gunzip(gz.name, overwrite = TRUE)
# process
cat('process. ', sep='')
# process the file ----
vnf.temp = fread(csv.name) %>%
rename_all(tolower) %>% # rename all columns to lowercase for convenience
select(all_of(vnf.cols)) %>% # collect relevant columns
na_if(999999) %>% # replace 999999 as missing
filter(!is.na(temp_bb)) %>% # keep those not missing temperature
# generate unique VNF_ID and valid date-format date column
mutate(vnf_id = paste0('VNF', gsub('-','',vnf.dates[d]), sprintf('%06d', 1:nrow(.))),
date = as.Date(substr(date_mscan, 1, 10), format = "%Y/%m/%d"),
file_date = vnf.dates[d]) %>%
# collect and rename certain columns, and drop a few
select(vnf_id, date, lon = lon_gmtco, lat = lat_gmtco,
everything(), -date_mscan) %>%
# filter by lat & lon bounds (bbox)
filter(lon >= lon_bounds[1], lon <= lon_bounds[2],
lat >= lat_bounds[1], lat <= lat_bounds[2])
# delete CSV file
file.remove(csv.name)
# report time took t to download and process ----
cat(difftime(Sys.time(), start_time, units='secs') %>% ceiling(),
" secs.\r", sep='')
}, error = function(e){
print(e)
})
# append to existing data
vnf = rbindlist(
list(vnf,
vnf.temp))
# output every 50 days. updating every day would be pretty slow
if(d %% 50 == 0 | d == length(vnf.dates)){
saveRDS(vnf, paste0(data.dir, "ef_vnf/ef-vnf.rds"))
}
}
pb.vnf = paste0(data.dir, "pb_vnf/pb-vnf.rds") %>% readRDS() %>%
# Filter for detections with strongest confidence (cloud mask = 0)
filter(cloud_mask == 0) %>%
# select and rename some columns
dplyr::select(vnf_id, date, vnf_lon = lon, vnf_lat = lat,
temp_bb, area_bb, rhi) %>%
filter(!(date < "2016-01-01"))
ef.vnf = paste0(data.dir, "ef_vnf/ef-vnf.rds") %>% readRDS() %>%
# Filter for detections with strongest confidence (cloud mask = 0)
filter(cloud_mask == 0) %>%
# select and rename some columns
dplyr::select(vnf_id, date, vnf_lon = lon, vnf_lat = lat,
temp_bb, area_bb, rhi) %>%
filter(!(date < "2016-01-01"))
data.dir = paste0(getwd(), '/data/')
code.dir = paste0(getwd())
ef_bbox = st_read("../JerryData/shapes/ShalePlays_US_EIA_Dec2021.shp") %>%
filter((Basin %in% c("Western Gulf"))) %>%
st_bbox()
ef_bbox = st_read("../JerryData/shapes/ShalePlays_US_EIA_Dec2021.shp") %>%
filter((Basin %in% c("Western Gulf"))) %>%
st_bbox()
ef_bbox = st_read("../JerryData/shapes/ShalePlays_US_EIA_Dec2021.shp") %>%
filter((Basin %in% c("Western Gulf"))) %>%
st_bbox()
ef_bbox = st_read("../JerryData/shapes/ShalePlays_US_EIA_Dec2021.shp") %>%
filter((Basin %in% c("Western Gulf"))) %>%
st_bbox()
library(dplyr)
ef_bbox = st_read("../JerryData/shapes/ShalePlays_US_EIA_Dec2021.shp") %>%
filter((Basin %in% c("Western Gulf"))) %>%
st_bbox()
library(dplyr)
library(httr)
library(jsonlite)
library(utils)
pkgs = c('data.table', 'R.utils', 'sf', 'tidyverse')
for(p in pkgs) require(p, character.only = TRUE)
rm(p, pkgs)
code.dir = paste0(getwd())
data.dir = paste0(getwd(), '/data/')
#EAGLE FORD!
ef_bbox = st_read("../JerryData/shapes/ShalePlays_US_EIA_Dec2021.shp") %>%
filter((Basin %in% c("Western Gulf"))) %>%
st_bbox()
ef_bbox = st_read("../JerryData/shapes/ShalePlays_US_EIA_Dec2021.shp")
library(dplyr)
non_eia_ef <- readRDS("JerryData/ef_vnf/ef-vnf.rds")
library(dplyr)
non_eia_ef <- readRDS("JerryData/ef_vnf/ef-vnf.rds")
non_eia_ef <- readRDS("JerryData/ef_vnf/ef-vnf.rds")
non_eia_ef <- readRDS("JerryData/ef_vnf/ef-vnf.rds")
non_eia_ef <- readRDS("JerryData/ef_vnf/ef-vnf.rds")
non_eia_pb <- readRDS("JerryData/pb_vnf/pb-vnf.rds")
draft_file <- file.choose()
x <- readRDS(draft_file)
View(x)
test <- x %>% filter(date <= "2017-01-01")
library(dplyr)
test <- x %>% filter(date <= "2017-01-01")
library(dplyr)
non_eia_ef <- readRDS("JerryData/ef_vnf/ef-vnf.rds")
test <- file.choose()
data <- read.RDS(test)
test <- file.choose()
data <- read.RDS(test)
setwd("~/Desktop")
data <- read.RDS(test)
test <- file.choose()
data <- readRDS(test)
View(data)
EF_vnf <- readRDS("../JerryData/ef_vnf/ef-vnf.rds")
setwd("~/Desktop/Current School Stuff/NSERC Shiny Application/Data/Data_Merging")
library(dplyr)
library(ggplot2)
library(tidyverse)
library(sf)
library(raster)
Counties_Spatial <- st_read("../KenData/counties.shp")
ZIP_Spatial <- st_read("../KenData/zips.shp")
ZIP_valid <- subset(ZIP_Spatial, st_is_valid(ZIP_Spatial))
#Loading Data from Jerry
EF_vnf <- readRDS("../JerryData/ef_vnf/ef-vnf.rds")
PB_vnf <- readRDS("../JerryData/pb_vnf/pb-vnf.rds")
joining_data <- st_as_sf(data, coords = c("lon", "lat"), crs = 4326)
joining_data <- st_as_sf(data, coords = c("vnf_lon", "vnf_lat"), crs = 4326)
View(joining_data)
Data <- readRDS("../JerryData/flare_full.rds")
Data_coord <-  st_as_sf(Data, coords = c("vnf_lon", "vnf_lat"), crs = 4326)
View(Data_coord)
Data_join <- st_join(Data_coord, Counties_Spatial)
View(Data_join)
x <- Data_join %>% filter(!is.na(county))
library(dplyr)
library(ggplot2)
library(tidyverse)
library(sf)
library(raster)
Counties_Spatial <- st_read("../KenData/counties.shp")
ZIP_Spatial <- st_read("../KenData/zips.shp")
ZIP_valid <- subset(ZIP_Spatial, st_is_valid(ZIP_Spatial))
#Loading Data from Jerry
EF_vnf <- readRDS("../JerryData/ef_vnf/ef-vnf.rds")
PB_vnf <- readRDS("../JerryData/pb_vnf/pb-vnf.rds")
#Converting Coordinates (Long/Lat) in Jerry's Data both EF_vnf and PB_vnf
EF_vnf_coordinates <- st_as_sf(EF_vnf, coords = c("lon", "lat"), crs = 4326)
PB_vnf_coordinates <- st_as_sf(PB_vnf, coords = c("lon", "lat"), crs = 4326)
#Joining Counties and Jerry's Data (Permian Basin)
PB_vnf_counties_join <- st_join(PB_vnf_coordinates, Counties_Spatial)
#Joining Counties and Jerry's Data(Eagle Ford)
EF_vnf_coordinates <- st_join(EF_vnf_coordinates, Counties_Spatial)
#Joining ZIP and Jerry's Data(Permian Basin) with Counties
PB_final <- st_join(PB_vnf_coordinates, ZIP_valid)
#Joining ZIP and Jerry's Data(Permian Basin) with Counties
EF_final <- st_join(EF_vnf_coordinates, ZIP_valid)
#Permian Basin
PB_final<- PB_final %>%st_transform(crs = 4326) %>%
mutate(
long = st_coordinates(.)[, "X"],
lat = st_coordinates(.)[, "Y"]
)%>% as.data.frame()
#Drop last column (EF)
PB_final <- PB_final[, -16]
#Eagle Ford
EF_final<- EF_final %>%st_transform(crs = 4326) %>%
mutate(
long = st_coordinates(.)[, "X"],
lat = st_coordinates(.)[, "Y"]
)%>% as.data.frame()
#Drop last column  (EF)
EF_final <- EF_final[, -21]
#Filtering out temperatures
PB_final <- PB_final %>% filter(temp_bb > 1600)
EF_final <- EF_final %>% filter(temp_bb > 1600)
#Selecting only columns of interest
PB_final <- PB_final %>% dplyr::select(vnf_id, date, long, lat, zip, state, basin, county)
EF_final <- EF_final %>% dplyr::select(vnf_id, date, county.y, zip, basin, long, lat, state.y)%>%rename(state = state.y, county = county.y)
View(PB_vnf)
View(PB_final)
PB_2016 <- PB_final %>% filter(date >= "2016-01-03")
EF_2016 <- EF_final %>% filter(date >= "2016-01-03")
test <- file.choose()
Ken  <- readRDS(test)
View(Ken)
library(dplyr)
library(ggplot2)
library(tidyverse)
library(sf)
library(raster)
Counties_Spatial <- st_read("../KenData/counties.shp")
ZIP_Spatial <- st_read("../KenData/zips.shp")
ZIP_valid <- subset(ZIP_Spatial, st_is_valid(ZIP_Spatial))
Data <- readRDS("../JerryData/flare_full.rds")
Data_coord <-  st_as_sf(Data, coords = c("vnf_lon", "vnf_lat"), crs = 4326)
Data_join <- st_join(Data_coord, Counties_Spatial)
View(Data_join)
Data_filter <- Data_join(temp_bb > 1600)
Data_filter <- Data_join %>% filter(temp_bb > 1600)
Data_filter <- Data_join %>% filter(temp_bb > 1600, cluster != 0)
View(Data_filter)
Data_final <-na.omit(Data_filter)
View(Data_final)
test <- file.choose()
Ken  <- readRDS(test)
View(Ken)
library(dplyr)
Ken <- Ken %>% filter(date >= "2016-01-03")
View(Ken)
Ken_2012_2020 <- readRDS("../KenData/vnf-2012-2020.rds")
Ken_2012_2015 <- Ken_2012_2020 %>% filter(!(date>= ("2016-01-03")))
View(Ken_2012_2015)
View(Ken_2012_2020)
Ken_2012_2016 <- Ken_2012_2020 %>% filter(!(date>= ("2016-01-03")), !(basin == "WILLISTON"))
Ken_2012_2016 <- Ken_2012_2020 %>% filter(!(date>= ("2016-01-03")))
Ken_2012_2016 <- Ken_2012_2020 %>% filter(!(date>= ("2016-01-03")), !(basin == "WILLISTON"))
library(dplyr)
library(ggplot2)
library(tidyverse)
library(sf)
library(raster)
#------------------------------------------------------IGNORE------------------------------------#
#Data was intially wrong
Counties_Spatial <- st_read("../KenData/counties.shp")
ZIP_Spatial <- st_read("../KenData/zips.shp")
ZIP_valid <- subset(ZIP_Spatial, st_is_valid(ZIP_Spatial))
#After receiving Jerry's Updated Data
Data_2016_2022 <- readRDS("../JerryData/flare_full.rds")
Data_coord_2016_2022 <-  st_as_sf(Data, coords = c("vnf_lon", "vnf_lat"), crs = 4326)
#Filtering out temperatures and noise
Data_filter_2016_2022 <- Data_join %>% filter(temp_bb > 1600, cluster != 0)
setwd("~/Desktop/Current School Stuff/NSERC Shiny Application/Data/Data_Merging")
library(dplyr)
library(ggplot2)
library(tidyverse)
library(sf)
library(raster)
#------------------------------------------------------IGNORE------------------------------------#
#Data was intially wrong
Counties_Spatial <- st_read("../KenData/counties.shp")
ZIP_Spatial <- st_read("../KenData/zips.shp")
ZIP_valid <- subset(ZIP_Spatial, st_is_valid(ZIP_Spatial))
#After receiving Jerry's Updated Data
Data_2016_2022 <- readRDS("../JerryData/flare_full.rds")
Data_coord_2016_2022 <-  st_as_sf(Data, coords = c("vnf_lon", "vnf_lat"), crs = 4326)
Data_coord_2016_2022 <-  st_as_sf(Data_2016_2022, coords = c("vnf_lon", "vnf_lat"), crs = 4326)
Data_2016_2022 <- readRDS("../JerryData/flare_full.rds")
library(dplyr)
library(ggplot2)
library(tidyverse)
library(sf)
library(raster)
#------------------------------------------------------IGNORE------------------------------------#
#Data was intially wrong
Counties_Spatial <- st_read("../KenData/counties.shp")
ZIP_Spatial <- st_read("../KenData/zips.shp")
ZIP_valid <- subset(ZIP_Spatial, st_is_valid(ZIP_Spatial))
setwd("~/Desktop/Current School Stuff/NSERC Shiny Application/Data/Data_Merging")
library(dplyr)
library(ggplot2)
library(tidyverse)
library(sf)
library(raster)
#------------------------------------------------------IGNORE------------------------------------#
#Data was intially wrong
Counties_Spatial <- st_read("../KenData/counties.shp")
ZIP_Spatial <- st_read("../KenData/zips.shp")
ZIP_valid <- subset(ZIP_Spatial, st_is_valid(ZIP_Spatial))
library(dplyr)
library(ggplot2)
library(tidyverse)
library(sf)
library(raster)
#------------------------------------------------------IGNORE------------------------------------#
#Data was intially wrong
Counties_Spatial <- st_read("../KenData/counties.shp")
ZIP_Spatial <- st_read("../KenData/zips.shp")
ZIP_valid <- subset(ZIP_Spatial, st_is_valid(ZIP_Spatial))
