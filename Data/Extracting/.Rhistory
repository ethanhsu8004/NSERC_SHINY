area_bb = numeric(),
cloud_mask = integer(),
file_date = as.Date(character())
)
# start new update from date after last date in existing data
start_date = as.Date("2022-09-01")
}
vnf.dates = seq(start_date, end_date, "days")
for (date in 1:length(data)){
url.name = paste0(vnfv30.url.pfx[1],
gsub('-', '', vnf.dates[date]), vnfv30.url.pfx[2])
gz.name = paste0(data.dir, '', basename(url.name))
csv.name = gsub(".gz", "", gz.name)
tryCatch({
cat("[", as.character(vnf.dates[date]),'] download. ', sep='')
download.file(url.name, gz.name,
mode = 'wb', quiet = TRUE,
headers = list(Authorization = auth))
R.utils::gunzip(gz.name, overwrite = TRUE)
vnf.temp = fread(csv.name) %>%
rename_all(tolower) %>% # rename all columns to lowercase for convenience
select(all_of(vnf.cols))
is.na(vnf.temp) <- vnf.temp == 999999
vnf.temp = vnf.temp %>% filter(!is.na(temp_bb)) %>%  mutate(vnf_id = paste0('VNF', gsub('-','',vnf.dates[date]), sprintf('%06d', 1:nrow(.))),
date = as.Date(substr(date_mscan, 1, 10), format = "%Y/%m/%d"),
file_date = vnf.dates[date]) %>% select(vnf_id, date, lon = lon_gmtco, lat = lat_gmtco,
everything(), -date_mscan) %>%
# filter by lat & lon bounds (bbox)
filter(lon >= lon_bounds[1], lon <= lon_bounds[2],
lat >= lat_bounds[1], lat <= lat_bounds[2])
vnf = rbindlist(
list(vnf,
vnf.temp))
}, error = function(e){
print(e)})
file.remove(csv.name)
}
for (date in 1:length(data)){
url.name = paste0(vnfv30.url.pfx[1],
gsub('-', '', vnf.dates[date]), vnfv30.url.pfx[2])
gz.name = paste0(data.dir, '', basename(url.name))
csv.name = gsub(".gz", "", gz.name)
tryCatch({
cat("[", as.character(vnf.dates[date]),'] download. ', sep='')
download.file(url.name, gz.name,
mode = 'wb', quiet = TRUE,
headers = list(Authorization = auth))
R.utils::gunzip(gz.name, overwrite = TRUE)
vnf.temp = fread(csv.name) %>%
rename_all(tolower) %>% # rename all columns to lowercase for convenience
select(all_of(vnf.cols))
is.na(vnf.temp) <- vnf.temp == 999999
vnf.temp = vnf.temp %>% filter(!is.na(temp_bb)) %>%  mutate(vnf_id = paste0('VNF', gsub('-','',vnf.dates[date]), sprintf('%06d', 1:nrow(.))),
date = as.Date(substr(date_mscan, 1, 10), format = "%Y/%m/%d"),
file_date = vnf.dates[date]) %>% select(vnf_id, date, lon = lon_gmtco, lat = lat_gmtco,
everything(), -date_mscan) %>%
# filter by lat & lon bounds (bbox)
filter(lon >= lon_bounds[1], lon <= lon_bounds[2],
lat >= lat_bounds[1], lat <= lat_bounds[2])
vnf = rbindlist(
list(vnf,
vnf.temp))
}, error = function(e){
print(e)})
file.remove(csv.name)
}
params <- list(
client_id = 'eogdata_oidc',
client_secret = '2677ad81-521b-4869-8480-6d05b9e57d48',
username = "ethan.hsu@mail.utoronto.ca",
password = "Qwe123qwe!",
grant_type = 'password'
)
token_url <- 'https://eogauth.mines.edu/auth/realms/master/protocol/openid-connect/token'
response <- POST(token_url, body = params, encode = "form")
access_token_list <- fromJSON(content(response,as="text",encoding="UTF-8"))
access_token <- access_token_list$access_token
data_url <- 'https://eogdata.mines.edu/eog/EOG_sensitive_contents'
auth <- paste('Bearer', access_token)
output_file <- basename(data_url)
download.file(data_url,output_file,mode = "wb", headers = list(Authorization = auth))
for (date in 1:length(data)){
url.name = paste0(vnfv30.url.pfx[1],
gsub('-', '', vnf.dates[date]), vnfv30.url.pfx[2])
gz.name = paste0(data.dir, '', basename(url.name))
csv.name = gsub(".gz", "", gz.name)
tryCatch({
cat("[", as.character(vnf.dates[date]),'] download. ', sep='')
download.file(url.name, gz.name,
mode = 'wb', quiet = TRUE,
headers = list(Authorization = auth))
R.utils::gunzip(gz.name, overwrite = TRUE)
vnf.temp = fread(csv.name) %>%
rename_all(tolower) %>% # rename all columns to lowercase for convenience
select(all_of(vnf.cols))
is.na(vnf.temp) <- vnf.temp == 999999
vnf.temp = vnf.temp %>% filter(!is.na(temp_bb)) %>%  mutate(vnf_id = paste0('VNF', gsub('-','',vnf.dates[date]), sprintf('%06d', 1:nrow(.))),
date = as.Date(substr(date_mscan, 1, 10), format = "%Y/%m/%d"),
file_date = vnf.dates[date]) %>% select(vnf_id, date, lon = lon_gmtco, lat = lat_gmtco,
everything(), -date_mscan) %>%
# filter by lat & lon bounds (bbox)
filter(lon >= lon_bounds[1], lon <= lon_bounds[2],
lat >= lat_bounds[1], lat <= lat_bounds[2])
vnf = rbindlist(
list(vnf,
vnf.temp))
}, error = function(e){
print(e)})
file.remove(csv.name)
}
for (date in 1:length(vnf.dates)){
url.name = paste0(vnfv30.url.pfx[1],
gsub('-', '', vnf.dates[date]), vnfv30.url.pfx[2])
gz.name = paste0(data.dir, '', basename(url.name))
csv.name = gsub(".gz", "", gz.name)
tryCatch({
cat("[", as.character(vnf.dates[date]),'] download. ', sep='')
download.file(url.name, gz.name,
mode = 'wb', quiet = TRUE,
headers = list(Authorization = auth))
R.utils::gunzip(gz.name, overwrite = TRUE)
vnf.temp = fread(csv.name) %>%
rename_all(tolower) %>% # rename all columns to lowercase for convenience
select(all_of(vnf.cols))
is.na(vnf.temp) <- vnf.temp == 999999
vnf.temp = vnf.temp %>% filter(!is.na(temp_bb)) %>%  mutate(vnf_id = paste0('VNF', gsub('-','',vnf.dates[date]), sprintf('%06d', 1:nrow(.))),
date = as.Date(substr(date_mscan, 1, 10), format = "%Y/%m/%d"),
file_date = vnf.dates[date]) %>% select(vnf_id, date, lon = lon_gmtco, lat = lat_gmtco,
everything(), -date_mscan) %>%
# filter by lat & lon bounds (bbox)
filter(lon >= lon_bounds[1], lon <= lon_bounds[2],
lat >= lat_bounds[1], lat <= lat_bounds[2])
vnf = rbindlist(
list(vnf,
vnf.temp))
}, error = function(e){
print(e)})
file.remove(csv.name)
}
file <- saveRDS("/ef_vnf", vnf)
file <- saveRDS(vnf, file = "ef_vnf.rds")
file <- saveRDS(vnf, file = "~/ef_vnf/ef_vnf.rds")
file <- saveRDS(vnf, file = "/ef_vnf/ef_vnf.rds")
file <- saveRDS(vnf, file = "ef_vnf/ef_vnf.rds")
pb_eia <- file.choose()
pb_eia_loaded <- readRDS(pb_eia)
View(pb_eia_loaded)
pb_non_eia <- file.choose()
pb_non_loaded <- readRDS(pb_non_eia)
View(pb_eia_loaded)
View(pb_non_loaded)
ddd <- na.omit(pb_non_loaded)
ken <- file.choose()
ken_loaded <- readRDS(ken)
View(ken_loaded)
Data <- readRDS("data/VNF_data_final_2012-2022.rds")
mine <- file.choose()
mine_loaded <- readRDS(mine)
View(mine_loaded)
View(ken_loaded)
View(pb_eia_loaded)
View(pb_non_loaded)
#Extracting from EagleFord
library(dplyr)
library(tidyverse)
library(httr)
library(jsonlite)
library(utils)
library(dbscan)
pkgs = c('data.table', 'R.utils', 'sf', 'tidyverse')
for(p in pkgs) require(p, character.only = TRUE)
rm(p, pkgs)
code.dir = paste0(getwd())
data.dir = paste0(getwd(), '/data/')
#EAGLE FORD!
ef_bbox = st_read("../JerryData/shapes/ShalePlays_US_EIA_Dec2021.shp") %>%
filter((Basin %in% c("Permian"))) %>%
st_bbox()
lon_bounds = ef_bbox[c(1,3)]; lat_bounds = ef_bbox[c(2,4)]
#This is just general guidelines from https://eogdata.mines.edu/products/register/ (bottom of page)
params <- list(
client_id = 'eogdata_oidc',
client_secret = '2677ad81-521b-4869-8480-6d05b9e57d48',
username = "ethan.hsu@mail.utoronto.ca",
password = "NSERC_stuff",
grant_type = 'password'
)
token_url <- 'https://eogauth.mines.edu/auth/realms/master/protocol/openid-connect/token'
response <- POST(token_url, body = params, encode = "form")
access_token_list <- fromJSON(content(response,as="text",encoding="UTF-8"))
access_token <- access_token_list$access_token
data_url <- 'https://eogdata.mines.edu/eog/EOG_sensitive_contents'
auth <- paste('Bearer', access_token)
output_file <- basename(data_url)
download.file(data_url,output_file,mode = "wb", headers = list(Authorization = auth))
#link to where the files are
vnfv30.url.pfx = c(
"https://eogdata.mines.edu/wwwdata/viirs_products/vnf/v30//VNF_npp_d",
"_noaa_v30-ez.csv.gz")
#columns that we want
vnf.cols = c('date_mscan', 'lon_gmtco', 'lat_gmtco', 'temp_bb', 'temp_bkg',
'esf_bb', 'rhi', 'rh', 'area_pixel', 'area_bb', 'cloud_mask')
#end date currently
end_date = as.Date("2023-06-15")
vnf.data.exist = file.exists(paste0(code.dir, "ef_vnf/ef-vnf.rds")) #change this line
if(vnf.data.exist){
vnf = readRDS(paste0(data.dir, "ef_vnf/ef-vnf.rds"))
# start new update from date after last date in existing data
start_date = max(vnf$file_date) + 1
} else {
vnf = data.table(
vnf_id = character(),
date = as.Date(character()),
lon = numeric(),
lat = numeric(),
temp_bb = integer(),
temp_bkg = integer(),
esf_bb = numeric(),
rhi = numeric(),
rh = numeric(),
area_pixel = numeric(),
area_bb = numeric(),
cloud_mask = integer(),
file_date = as.Date(character())
)
# start new update from date after last date in existing data
start_date = as.Date("2022-09-01")
}
vnf.dates = seq(start_date, end_date, "days")
for (date in 1:length(vnf.dates)){
url.name = paste0(vnfv30.url.pfx[1],
gsub('-', '', vnf.dates[date]), vnfv30.url.pfx[2])
gz.name = paste0(data.dir, '', basename(url.name))
csv.name = gsub(".gz", "", gz.name)
tryCatch({
cat("[", as.character(vnf.dates[date]),'] download. ', sep='')
download.file(url.name, gz.name,
mode = 'wb', quiet = TRUE,
headers = list(Authorization = auth))
R.utils::gunzip(gz.name, overwrite = TRUE)
vnf.temp = fread(csv.name) %>%
rename_all(tolower) %>% # rename all columns to lowercase for convenience
select(all_of(vnf.cols))
is.na(vnf.temp) <- vnf.temp == 999999
vnf.temp = vnf.temp %>% filter(!is.na(temp_bb)) %>%  mutate(vnf_id = paste0('VNF', gsub('-','',vnf.dates[date]), sprintf('%06d', 1:nrow(.))),
date = as.Date(substr(date_mscan, 1, 10), format = "%Y/%m/%d"),
file_date = vnf.dates[date]) %>% select(vnf_id, date, lon = lon_gmtco, lat = lat_gmtco,
everything(), -date_mscan) %>%
# filter by lat & lon bounds (bbox)
filter(lon >= lon_bounds[1], lon <= lon_bounds[2],
lat >= lat_bounds[1], lat <= lat_bounds[2])
vnf = rbindlist(
list(vnf,
vnf.temp))
}, error = function(e){
print(e)})
file.remove(csv.name)
}
#Extracting from EagleFord
library(dplyr)
library(tidyverse)
library(httr)
library(jsonlite)
library(utils)
library(dbscan)
pkgs = c('data.table', 'R.utils', 'sf', 'tidyverse')
for(p in pkgs) require(p, character.only = TRUE)
rm(p, pkgs)
code.dir = paste0(getwd())
data.dir = paste0(getwd(), '/data/')
#EAGLE FORD!
ef_bbox = st_read("../JerryData/shapes/ShalePlays_US_EIA_Dec2021.shp") %>%
filter((Basin %in% c("Permian"))) %>%
st_bbox()
lon_bounds = ef_bbox[c(1,3)]; lat_bounds = ef_bbox[c(2,4)]
#This is just general guidelines from https://eogdata.mines.edu/products/register/ (bottom of page)
params <- list(
client_id = 'eogdata_oidc',
client_secret = '2677ad81-521b-4869-8480-6d05b9e57d48',
username = "ethan.hsu@mail.utoronto.ca",
password = "NSERC_stuff",
grant_type = 'password'
)
token_url <- 'https://eogauth.mines.edu/auth/realms/master/protocol/openid-connect/token'
response <- POST(token_url, body = params, encode = "form")
access_token_list <- fromJSON(content(response,as="text",encoding="UTF-8"))
access_token <- access_token_list$access_token
data_url <- 'https://eogdata.mines.edu/eog/EOG_sensitive_contents'
auth <- paste('Bearer', access_token)
output_file <- basename(data_url)
download.file(data_url,output_file,mode = "wb", headers = list(Authorization = auth))
#link to where the files are
vnfv30.url.pfx = c(
"https://eogdata.mines.edu/wwwdata/viirs_products/vnf/v30//VNF_npp_d",
"_noaa_v30-ez.csv.gz")
#columns that we want
vnf.cols = c('date_mscan', 'lon_gmtco', 'lat_gmtco', 'temp_bb', 'temp_bkg',
'esf_bb', 'rhi', 'rh', 'area_pixel', 'area_bb', 'cloud_mask')
#end date currently
end_date = as.Date("2023-06-15")
vnf.data.exist = file.exists(paste0(code.dir, "ef_vnf/ef-vnf.rds")) #change this line
if(vnf.data.exist){
vnf = readRDS(paste0(data.dir, "ef_vnf/ef-vnf.rds"))
# start new update from date after last date in existing data
start_date = max(vnf$file_date) + 1
} else {
vnf = data.table(
vnf_id = character(),
date = as.Date(character()),
lon = numeric(),
lat = numeric(),
temp_bb = integer(),
temp_bkg = integer(),
esf_bb = numeric(),
rhi = numeric(),
rh = numeric(),
area_pixel = numeric(),
area_bb = numeric(),
cloud_mask = integer(),
file_date = as.Date(character())
)
# start new update from date after last date in existing data
start_date = as.Date("2022-09-01")
}
vnf.dates = seq(start_date, end_date, "days")
for (date in 1:length(vnf.dates)){
url.name = paste0(vnfv30.url.pfx[1],
gsub('-', '', vnf.dates[date]), vnfv30.url.pfx[2])
gz.name = paste0(data.dir, '', basename(url.name))
csv.name = gsub(".gz", "", gz.name)
tryCatch({
cat("[", as.character(vnf.dates[date]),'] download. ', sep='')
download.file(url.name, gz.name,
mode = 'wb', quiet = TRUE,
headers = list(Authorization = auth))
R.utils::gunzip(gz.name, overwrite = TRUE)
vnf.temp = fread(csv.name) %>%
rename_all(tolower) %>% # rename all columns to lowercase for convenience
select(all_of(vnf.cols))
is.na(vnf.temp) <- vnf.temp == 999999
vnf.temp = vnf.temp %>% filter(!is.na(temp_bb)) %>%  mutate(vnf_id = paste0('VNF', gsub('-','',vnf.dates[date]), sprintf('%06d', 1:nrow(.))),
date = as.Date(substr(date_mscan, 1, 10), format = "%Y/%m/%d"),
file_date = vnf.dates[date]) %>% select(vnf_id, date, lon = lon_gmtco, lat = lat_gmtco,
everything(), -date_mscan) %>%
# filter by lat & lon bounds (bbox)
filter(lon >= lon_bounds[1], lon <= lon_bounds[2],
lat >= lat_bounds[1], lat <= lat_bounds[2])
vnf = rbindlist(
list(vnf,
vnf.temp))
}, error = function(e){
print(e)})
file.remove(csv.name)
}
file <- saveRDS(vnf, file = "pb_vnf/ef_vnf.rds")
file <- saveRDS(vnf, file = "pb_vnf/ef_vnf.rds")
file <- saveRDS(vnf, file = "pb_vnf/pb_vnf.rds")
View(vnf)
test <- file.choose()
zzz <- readRDS(test)
View(zzz)
ef <- file.choose()
ef_j <- readRDS(ef)
View(ef_j)
View(zzz)
View(pb_eia_loaded)
View(pb_non_loaded)
pb_eia_loaded <- readRDS(pb_eia)
pb_non_loaded <- readRDS(pb_non_eia)
View(pb_eia_loaded)
View(pb_non_loaded)
filtered_out_date <- pb_non_loaded %>% filter(date >= "2016-01-03")
View(filtered_out_date)
filtered_out_date <- pb_non_loaded %>% filter(date >= "2016-01-03") %>% filter(cloud_mask == 0)
filtered_out_date <- pb_non_loaded %>% filter(date >= "2016-01-03") %>% filter(cloud_mask == 0) %>% filter(temp_bb >= 1600)
View(params)
# packages ----------------------------------------------------------------
pkgs = c('data.table', 'dbscan', 'sf', 'tidyverse')
for(p in pkgs) require(p, character.only = TRUE)
rm(p, pkgs)
# directories -------------------------------------------------------------
code.dir = paste0(getwd())
data.dir = paste0(getwd())
pkgs = c('data.table', 'R.utils', 'sf', 'tidyverse')
for(p in pkgs) require(p, character.only = TRUE)
rm(p, pkgs)
pkgs = c("httr", "jsonlite", "utils")
for(p in pkgs) require(p, character.only = TRUE)
# Retrieve access token
# Retrieve access token
params <- list(
client_id = 'eogdata_oidc',
client_secret = '2677ad81-521b-4869-8480-6d05b9e57d48',
username = "jerryp.wu@mail.utoronto.ca",
password = "kBREu_D63qZ34g4",
grant_type = 'password'
)
token_url <- 'https://eogauth.mines.edu/auth/realms/master/protocol/openid-connect/token'
response <- POST(token_url, body = params, encode = "form")
access_token_list <- fromJSON(content(response,as="text",encoding="UTF-8"))
access_token <- access_token_list$access_token
auth <- paste('Bearer', access_token)
vnfv21.url.pfx = c(
"https://eogdata.mines.edu/wwwdata/viirs_products/vnf/v21//VNF_npp_d",
"_noaa_v21.csv.gz")
vnfv30.url.pfx = c(
"https://eogdata.mines.edu/wwwdata/viirs_products/vnf/v30//VNF_npp_d",
"_noaa_v30-ez.csv.gz")
for(p in pkgs) require(p, character.only = TRUE)
rm(p, pkgs)
# directories -------------------------------------------------------------
source(paste0(code.dir, "helper_functions.R"))
if(vnf.data.exist){
vnf = readRDS(paste0(data.dir, "ef_vnf/ef-vnf.rds"))
# start new update from date after last date in existing data
start_date = max(vnf$file_date) + 1
} else {
vnf = data.table(
vnf_id = character(),
date = as.Date(character()),
lon = numeric(),
lat = numeric(),
temp_bb = integer(),
temp_bkg = integer(),
esf_bb = numeric(),
rhi = numeric(),
rh = numeric(),
area_pixel = numeric(),
area_bb = numeric(),
cloud_mask = integer(),
file_date = as.Date(character())
)
# start new update from date after last date in existing data
start_date = as.Date("2012-03-01")
}
# vector of dates
if(start_date > end_date){
stop("Invalid dates for update.")
} else{
vnf.dates = seq(start_date, end_date, "days")
}
vnf.data.exist = file.exists(paste0(data.dir, "ef_vnf/ef-vnf.rds"))
end_date = as.Date("2022-08-31")
if(vnf.data.exist){
vnf = readRDS(paste0(data.dir, "ef_vnf/ef-vnf.rds"))
# start new update from date after last date in existing data
start_date = max(vnf$file_date) + 1
} else {
vnf = data.table(
vnf_id = character(),
date = as.Date(character()),
lon = numeric(),
lat = numeric(),
temp_bb = integer(),
temp_bkg = integer(),
esf_bb = numeric(),
rhi = numeric(),
rh = numeric(),
area_pixel = numeric(),
area_bb = numeric(),
cloud_mask = integer(),
file_date = as.Date(character())
)
# start new update from date after last date in existing data
start_date = as.Date("2012-03-01")
}
# vector of dates
if(start_date > end_date){
stop("Invalid dates for update.")
} else{
vnf.dates = seq(start_date, end_date, "days")
}
eia.pb.vnf = pb.vnf %>% filter(temp_bb >= 1600)
pb.ef = paste0(data.dir, "shapes/ShalePlays_US_EIA_Dec2021.shp") %>%
st_read(quiet = TRUE) %>%
filter((Basin %in% c("Permian", "Western Gulf")))
pb.vnf = paste0(data.dir, "pb_vnf/pb-vnf.rds") %>% readRDS() %>%
# Filter for detections with strongest confidence (cloud mask = 0)
filter(cloud_mask == 0) %>%
# select and rename some columns
dplyr::select(vnf_id, date, vnf_lon = lon, vnf_lat = lat,
temp_bb, area_bb, rhi) %>%
filter(!(date < "2016-01-01"))
ef_eia <- file.choose()
ef_non_eia <- file.choose()
ef_non_eia_loaded <- file.choose()
ef_non_eia_loaded <- readRDS()
ef_non_eia_loaded <- readRDS(ef_non_eia)
ef_eia <- file.choose()
ef_eia_loaded <- readRDS(ef_eia)
ef_final <- ef_non_eia_loaded %>% filter(date >= "2016-01-03") %>% filter(cloud_mask == 0) %>% filter(temp_bb >= 1600)
View(ef_final)
View(ef_non_eia_loaded)
View(ef_eia_loaded)
ef_final <- ef_non_eia_loaded %>% filter(date >= "2016-01-03") %>% filter(cloud_mask == 0) %>% filter(temp_bb >= 1600) %>% select(vnf_id, date, lon, lat, temp_bb, area_bb, rhi)
print(ef_eia_loaded == ef_final)
all.equal(ef_final, ef_eia_loaded)
ef_final [0] = 2
ef_final[0] = 2
ef_final <- ef_non_eia_loaded %>% filter(date >= "2016-01-03") %>% filter(cloud_mask == 0) %>%
filter(temp_bb >= 1600) %>% select(vnf_id, date, lon, lat, temp_bb, area_bb, rhi) %>%
rename(vnf_lon = lon, vnf_lat = lat)
ef_final <- ef_non_eia_loaded %>% filter(date >= "2016-01-03") %>% filter(cloud_mask == 0) %>%
filter(temp_bb >= 1600) %>% select(vnf_id, date, lon, lat, temp_bb, area_bb, rhi)
ef_non_eia_loaded <- readRDS(ef_non_eia)
ef_non_eia_loaded <- readRDS(ef_non_eia)
filtered_out_date <- pb_non_loaded %>% filter(date >= "2016-01-03") %>% filter(cloud_mask == 0) %>% filter(temp_bb >= 1600)
pb_non_loaded <- readRDS(pb_non_eia)
ef_eia_loaded <- readRDS(ef_eia)
ef_eia <- file.choose()
library(dplyr)
library(dplyr)
